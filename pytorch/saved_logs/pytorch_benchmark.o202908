---------------------------------
Using NVIDIA_NGC runtime.
Thu Mar 2 08:45:58 EST 2023
---------------------------------
INFO:    underlay of /etc/localtime required more than 50 (94) bind mounts
INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (470) bind mounts
Training with a single process on 1 GPUs.
Model resnet50d created, param count:25576264
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 1
Train: 0 [   0/2502 (  0%)]  Loss: 6.977 (6.98)  Time: 22.073s,   23.20/s  (22.073s,   23.20/s)  LR: 4.000e-01  Data: 4.293 (4.293)
Train: 0 [  50/2502 (  2%)]  Loss: 6.757 (6.93)  Time: 0.452s, 1133.73/s  (0.878s,  583.08/s)  LR: 4.000e-01  Data: 0.020 (0.106)
Train: 0 [ 100/2502 (  4%)]  Loss: 6.608 (6.78)  Time: 0.458s, 1118.85/s  (0.668s,  766.05/s)  LR: 4.000e-01  Data: 0.024 (0.064)
Train: 0 [ 150/2502 (  6%)]  Loss: 6.430 (6.69)  Time: 0.453s, 1129.24/s  (0.598s,  856.77/s)  LR: 4.000e-01  Data: 0.020 (0.050)
Train: 0 [ 200/2502 (  8%)]  Loss: 6.341 (6.62)  Time: 0.456s, 1122.73/s  (0.562s,  910.70/s)  LR: 4.000e-01  Data: 0.020 (0.043)
Train: 0 [ 250/2502 ( 10%)]  Loss: 6.324 (6.55)  Time: 0.459s, 1114.74/s  (0.541s,  946.32/s)  LR: 4.000e-01  Data: 0.023 (0.038)
Train: 0 [ 300/2502 ( 12%)]  Loss: 6.180 (6.49)  Time: 0.456s, 1122.04/s  (0.527s,  971.52/s)  LR: 4.000e-01  Data: 0.020 (0.035)
Train: 0 [ 350/2502 ( 14%)]  Loss: 6.019 (6.44)  Time: 0.457s, 1119.39/s  (0.517s,  990.26/s)  LR: 4.000e-01  Data: 0.020 (0.033)
Train: 0 [ 400/2502 ( 16%)]  Loss: 5.966 (6.38)  Time: 0.459s, 1114.60/s  (0.510s, 1004.77/s)  LR: 4.000e-01  Data: 0.023 (0.032)
Train: 0 [ 450/2502 ( 18%)]  Loss: 5.901 (6.33)  Time: 0.457s, 1120.20/s  (0.504s, 1016.25/s)  LR: 4.000e-01  Data: 0.020 (0.030)
Train: 0 [ 500/2502 ( 20%)]  Loss: 5.775 (6.28)  Time: 0.457s, 1120.48/s  (0.499s, 1025.61/s)  LR: 4.000e-01  Data: 0.020 (0.029)
Train: 0 [ 550/2502 ( 22%)]  Loss: 5.703 (6.23)  Time: 0.461s, 1111.32/s  (0.495s, 1033.34/s)  LR: 4.000e-01  Data: 0.023 (0.029)
Train: 0 [ 600/2502 ( 24%)]  Loss: 5.587 (6.19)  Time: 0.456s, 1121.77/s  (0.492s, 1039.78/s)  LR: 4.000e-01  Data: 0.020 (0.028)
Train: 0 [ 650/2502 ( 26%)]  Loss: 5.506 (6.14)  Time: 0.457s, 1119.81/s  (0.490s, 1045.37/s)  LR: 4.000e-01  Data: 0.020 (0.028)
Train: 0 [ 700/2502 ( 28%)]  Loss: 5.451 (6.10)  Time: 0.460s, 1112.03/s  (0.488s, 1050.21/s)  LR: 4.000e-01  Data: 0.023 (0.027)
Train: 0 [ 750/2502 ( 30%)]  Loss: 5.379 (6.06)  Time: 0.458s, 1118.95/s  (0.486s, 1054.45/s)  LR: 4.000e-01  Data: 0.020 (0.027)
Train: 0 [ 800/2502 ( 32%)]  Loss: 5.278 (6.02)  Time: 0.459s, 1116.41/s  (0.484s, 1058.01/s)  LR: 4.000e-01  Data: 0.020 (0.026)
Train: 0 [ 850/2502 ( 34%)]  Loss: 5.285 (5.98)  Time: 0.461s, 1110.15/s  (0.482s, 1061.21/s)  LR: 4.000e-01  Data: 0.024 (0.026)
Train: 0 [ 900/2502 ( 36%)]  Loss: 5.206 (5.94)  Time: 0.462s, 1109.18/s  (0.481s, 1063.96/s)  LR: 4.000e-01  Data: 0.021 (0.026)
Train: 0 [ 950/2502 ( 38%)]  Loss: 5.356 (5.90)  Time: 0.458s, 1118.02/s  (0.480s, 1066.44/s)  LR: 4.000e-01  Data: 0.020 (0.026)
Train: 0 [1000/2502 ( 40%)]  Loss: 5.150 (5.87)  Time: 0.461s, 1110.72/s  (0.479s, 1068.65/s)  LR: 4.000e-01  Data: 0.023 (0.025)
Train: 0 [1050/2502 ( 42%)]  Loss: 4.960 (5.83)  Time: 0.459s, 1114.38/s  (0.478s, 1070.69/s)  LR: 4.000e-01  Data: 0.021 (0.025)
Train: 0 [1100/2502 ( 44%)]  Loss: 5.062 (5.80)  Time: 0.458s, 1118.50/s  (0.477s, 1072.64/s)  LR: 4.000e-01  Data: 0.020 (0.025)
Train: 0 [1150/2502 ( 46%)]  Loss: 5.036 (5.76)  Time: 0.461s, 1111.71/s  (0.477s, 1074.13/s)  LR: 4.000e-01  Data: 0.023 (0.025)
Train: 0 [1200/2502 ( 48%)]  Loss: 5.000 (5.73)  Time: 0.464s, 1102.54/s  (0.476s, 1075.62/s)  LR: 4.000e-01  Data: 0.024 (0.025)
Train: 0 [1250/2502 ( 50%)]  Loss: 4.922 (5.70)  Time: 0.458s, 1117.02/s  (0.475s, 1076.98/s)  LR: 4.000e-01  Data: 0.020 (0.025)
Train: 0 [1300/2502 ( 52%)]  Loss: 4.877 (5.67)  Time: 0.461s, 1110.76/s  (0.475s, 1078.40/s)  LR: 4.000e-01  Data: 0.023 (0.025)
Train: 0 [1350/2502 ( 54%)]  Loss: 4.787 (5.63)  Time: 0.461s, 1111.52/s  (0.474s, 1079.70/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1400/2502 ( 56%)]  Loss: 4.803 (5.60)  Time: 0.459s, 1116.62/s  (0.474s, 1080.86/s)  LR: 4.000e-01  Data: 0.020 (0.024)
Train: 0 [1450/2502 ( 58%)]  Loss: 4.668 (5.57)  Time: 0.460s, 1112.17/s  (0.473s, 1082.02/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1500/2502 ( 60%)]  Loss: 4.733 (5.55)  Time: 0.461s, 1111.49/s  (0.473s, 1083.10/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1550/2502 ( 62%)]  Loss: 4.864 (5.52)  Time: 0.458s, 1118.26/s  (0.472s, 1084.13/s)  LR: 4.000e-01  Data: 0.020 (0.024)
Train: 0 [1600/2502 ( 64%)]  Loss: 4.805 (5.49)  Time: 0.462s, 1109.01/s  (0.472s, 1085.09/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1650/2502 ( 66%)]  Loss: 4.596 (5.46)  Time: 0.462s, 1109.12/s  (0.471s, 1085.97/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1700/2502 ( 68%)]  Loss: 4.442 (5.44)  Time: 0.458s, 1118.77/s  (0.471s, 1086.78/s)  LR: 4.000e-01  Data: 0.020 (0.024)
Train: 0 [1750/2502 ( 70%)]  Loss: 4.599 (5.41)  Time: 0.460s, 1113.72/s  (0.471s, 1087.50/s)  LR: 4.000e-01  Data: 0.024 (0.024)
Train: 0 [1800/2502 ( 72%)]  Loss: 4.526 (5.39)  Time: 0.461s, 1110.89/s  (0.471s, 1088.19/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1850/2502 ( 74%)]  Loss: 4.458 (5.36)  Time: 0.458s, 1117.27/s  (0.470s, 1088.85/s)  LR: 4.000e-01  Data: 0.020 (0.024)
Train: 0 [1900/2502 ( 76%)]  Loss: 4.396 (5.34)  Time: 0.461s, 1110.51/s  (0.470s, 1089.51/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [1950/2502 ( 78%)]  Loss: 4.475 (5.32)  Time: 0.460s, 1112.72/s  (0.470s, 1090.15/s)  LR: 4.000e-01  Data: 0.024 (0.024)
Train: 0 [2000/2502 ( 80%)]  Loss: 4.430 (5.29)  Time: 0.458s, 1118.72/s  (0.469s, 1090.76/s)  LR: 4.000e-01  Data: 0.020 (0.024)
Train: 0 [2050/2502 ( 82%)]  Loss: 4.312 (5.27)  Time: 0.461s, 1111.40/s  (0.469s, 1091.34/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [2100/2502 ( 84%)]  Loss: 4.400 (5.25)  Time: 0.461s, 1111.45/s  (0.469s, 1091.89/s)  LR: 4.000e-01  Data: 0.023 (0.024)
Train: 0 [2150/2502 ( 86%)]  Loss: 4.348 (5.23)  Time: 0.466s, 1099.48/s  (0.469s, 1092.42/s)  LR: 4.000e-01  Data: 0.029 (0.024)
Train: 0 [2200/2502 ( 88%)]  Loss: 4.240 (5.20)  Time: 0.464s, 1103.01/s  (0.469s, 1092.75/s)  LR: 4.000e-01  Data: 0.024 (0.023)
Train: 0 [2250/2502 ( 90%)]  Loss: 4.141 (5.18)  Time: 0.461s, 1111.30/s  (0.468s, 1093.19/s)  LR: 4.000e-01  Data: 0.023 (0.023)
Train: 0 [2300/2502 ( 92%)]  Loss: 4.126 (5.16)  Time: 0.457s, 1119.32/s  (0.468s, 1093.68/s)  LR: 4.000e-01  Data: 0.020 (0.023)
Train: 0 [2350/2502 ( 94%)]  Loss: 4.194 (5.14)  Time: 0.460s, 1112.10/s  (0.468s, 1094.14/s)  LR: 4.000e-01  Data: 0.023 (0.023)
Train: 0 [2400/2502 ( 96%)]  Loss: 4.180 (5.12)  Time: 0.460s, 1112.30/s  (0.468s, 1094.58/s)  LR: 4.000e-01  Data: 0.023 (0.023)
Train: 0 [2450/2502 ( 98%)]  Loss: 4.081 (5.10)  Time: 0.457s, 1120.17/s  (0.468s, 1095.00/s)  LR: 4.000e-01  Data: 0.020 (0.023)
Train: 0 [2500/2502 (100%)]  Loss: 4.160 (5.09)  Time: 0.457s, 1119.16/s  (0.467s, 1095.37/s)  LR: 4.000e-01  Data: 0.022 (0.023)
Train: 0 [2501/2502 (100%)]  Loss: 4.130 (5.09)  Time: 0.436s, 1175.31/s  (0.467s, 1095.40/s)  LR: 4.000e-01  Data: 0.000 (0.023)
Test: [   0/97]  Time: 5.765 (5.765)  Loss:  3.1387 (3.1387)  Acc@1: 32.4219 (32.4219)  Acc@5: 61.9141 (61.9141)
Test: [  50/97]  Time: 0.136 (0.365)  Loss:  4.4688 (3.8990)  Acc@1: 18.7500 (21.6682)  Acc@5: 34.7656 (44.5159)
Test: [  97/97]  Time: 1.349 (0.320)  Loss:  3.7227 (3.9186)  Acc@1: 27.3810 (22.0760)  Acc@5: 49.7024 (44.5460)
/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Current checkpoints:
 ('./output/train/20230302-134632-resnet50d-224/checkpoint-0.pth.tar', 22.076000003051757)

*** Best metric: 22.076000003051757 (epoch 0)
Total runtime (s): 1235
 
 
+------------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES REQUESTED | 
+------------------------------------------+ 
 
mem=372gb,walltime=02:00:00,ncpus=56
 
 
+-------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES USED | 
+-------------------------------------+ 
 
cput=02:12:50,mem=166749848kb,walltime=00:20:42,ncpus=56,cpupercent=631,vmem=1134650928kb
 
 
