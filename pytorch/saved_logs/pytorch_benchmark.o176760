---------------------------------
Using NVIDIA_NGC runtime.
---------------------------------
INFO:    underlay of /etc/localtime required more than 50 (94) bind mounts
INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (470) bind mounts
Training with a single process on 1 GPUs.
Model resnet50d created, param count:25576264
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 1
Train: 0 [   0/2502 (  0%)]  Loss: 6.977 (6.98)  Time: 31.429s,   16.29/s  (31.429s,   16.29/s)  LR: 4.000e-01  Data: 5.600 (5.600)
Train: 0 [  50/2502 (  2%)]  Loss: 6.758 (6.93)  Time: 0.472s, 1083.63/s  (1.071s,  477.85/s)  LR: 4.000e-01  Data: 0.031 (0.133)
Train: 0 [ 100/2502 (  4%)]  Loss: 6.612 (6.78)  Time: 0.466s, 1098.51/s  (0.771s,  664.07/s)  LR: 4.000e-01  Data: 0.024 (0.078)
Train: 0 [ 150/2502 (  6%)]  Loss: 6.463 (6.70)  Time: 0.464s, 1104.10/s  (0.670s,  764.31/s)  LR: 4.000e-01  Data: 0.020 (0.060)
Train: 0 [ 200/2502 (  8%)]  Loss: 6.369 (6.62)  Time: 0.475s, 1077.83/s  (0.619s,  826.62/s)  LR: 4.000e-01  Data: 0.030 (0.051)
Train: 0 [ 250/2502 ( 10%)]  Loss: 6.308 (6.56)  Time: 0.469s, 1091.48/s  (0.589s,  869.01/s)  LR: 4.000e-01  Data: 0.024 (0.045)
Train: 0 [ 300/2502 ( 12%)]  Loss: 6.202 (6.50)  Time: 0.466s, 1099.15/s  (0.569s,  899.81/s)  LR: 4.000e-01  Data: 0.021 (0.042)
Train: 0 [ 350/2502 ( 14%)]  Loss: 6.044 (6.45)  Time: 0.468s, 1095.13/s  (0.554s,  923.45/s)  LR: 4.000e-01  Data: 0.022 (0.039)
Train: 0 [ 400/2502 ( 16%)]  Loss: 6.010 (6.40)  Time: 0.469s, 1091.08/s  (0.544s,  941.81/s)  LR: 4.000e-01  Data: 0.024 (0.037)
Train: 0 [ 450/2502 ( 18%)]  Loss: 5.929 (6.34)  Time: 0.467s, 1096.74/s  (0.535s,  956.58/s)  LR: 4.000e-01  Data: 0.021 (0.035)
Train: 0 [ 500/2502 ( 20%)]  Loss: 5.794 (6.30)  Time: 0.467s, 1096.86/s  (0.529s,  968.73/s)  LR: 4.000e-01  Data: 0.021 (0.034)
Train: 0 [ 550/2502 ( 22%)]  Loss: 5.722 (6.25)  Time: 0.470s, 1089.52/s  (0.523s,  978.59/s)  LR: 4.000e-01  Data: 0.024 (0.033)
Train: 0 [ 600/2502 ( 24%)]  Loss: 5.611 (6.20)  Time: 0.468s, 1093.59/s  (0.519s,  987.23/s)  LR: 4.000e-01  Data: 0.021 (0.032)
Train: 0 [ 650/2502 ( 26%)]  Loss: 5.549 (6.16)  Time: 0.466s, 1099.09/s  (0.515s,  994.52/s)  LR: 4.000e-01  Data: 0.020 (0.031)
Train: 0 [ 700/2502 ( 28%)]  Loss: 5.483 (6.12)  Time: 0.470s, 1088.36/s  (0.512s, 1000.90/s)  LR: 4.000e-01  Data: 0.024 (0.031)
Train: 0 [ 750/2502 ( 30%)]  Loss: 5.416 (6.07)  Time: 0.479s, 1069.39/s  (0.509s, 1006.22/s)  LR: 4.000e-01  Data: 0.029 (0.030)
Train: 0 [ 800/2502 ( 32%)]  Loss: 5.307 (6.03)  Time: 0.467s, 1096.57/s  (0.506s, 1011.24/s)  LR: 4.000e-01  Data: 0.020 (0.030)
Train: 0 [ 850/2502 ( 34%)]  Loss: 5.308 (5.99)  Time: 0.471s, 1087.03/s  (0.504s, 1015.58/s)  LR: 4.000e-01  Data: 0.024 (0.029)
Train: 0 [ 900/2502 ( 36%)]  Loss: 5.218 (5.95)  Time: 0.467s, 1096.60/s  (0.502s, 1019.61/s)  LR: 4.000e-01  Data: 0.020 (0.029)
Train: 0 [ 950/2502 ( 38%)]  Loss: 5.357 (5.92)  Time: 0.466s, 1097.77/s  (0.500s, 1023.16/s)  LR: 4.000e-01  Data: 0.020 (0.029)
Train: 0 [1000/2502 ( 40%)]  Loss: 5.150 (5.88)  Time: 0.472s, 1085.59/s  (0.499s, 1026.26/s)  LR: 4.000e-01  Data: 0.024 (0.028)
Train: 0 [1050/2502 ( 42%)]  Loss: 4.983 (5.85)  Time: 0.482s, 1063.22/s  (0.498s, 1028.95/s)  LR: 4.000e-01  Data: 0.033 (0.028)
Train: 0 [1100/2502 ( 44%)]  Loss: 5.037 (5.81)  Time: 0.479s, 1069.97/s  (0.496s, 1031.22/s)  LR: 4.000e-01  Data: 0.029 (0.028)
Train: 0 [1150/2502 ( 46%)]  Loss: 5.088 (5.78)  Time: 0.473s, 1081.88/s  (0.495s, 1033.39/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [1200/2502 ( 48%)]  Loss: 4.991 (5.74)  Time: 0.475s, 1078.67/s  (0.495s, 1035.26/s)  LR: 4.000e-01  Data: 0.029 (0.028)
Train: 0 [1250/2502 ( 50%)]  Loss: 4.881 (5.71)  Time: 0.468s, 1093.34/s  (0.494s, 1037.19/s)  LR: 4.000e-01  Data: 0.021 (0.028)
Train: 0 [1300/2502 ( 52%)]  Loss: 4.916 (5.68)  Time: 0.474s, 1079.05/s  (0.493s, 1038.78/s)  LR: 4.000e-01  Data: 0.028 (0.028)
Train: 0 [1350/2502 ( 54%)]  Loss: 4.754 (5.65)  Time: 0.479s, 1069.38/s  (0.492s, 1040.18/s)  LR: 4.000e-01  Data: 0.030 (0.028)
Train: 0 [1400/2502 ( 56%)]  Loss: 4.783 (5.62)  Time: 0.476s, 1075.24/s  (0.492s, 1041.54/s)  LR: 4.000e-01  Data: 0.028 (0.028)
Train: 0 [1450/2502 ( 58%)]  Loss: 4.677 (5.59)  Time: 0.470s, 1089.52/s  (0.491s, 1042.87/s)  LR: 4.000e-01  Data: 0.024 (0.028)
Train: 0 [1500/2502 ( 60%)]  Loss: 4.775 (5.56)  Time: 0.476s, 1075.21/s  (0.490s, 1044.14/s)  LR: 4.000e-01  Data: 0.029 (0.028)
Train: 0 [1550/2502 ( 62%)]  Loss: 4.805 (5.53)  Time: 0.466s, 1098.02/s  (0.490s, 1045.35/s)  LR: 4.000e-01  Data: 0.021 (0.028)
Train: 0 [1600/2502 ( 64%)]  Loss: 4.812 (5.50)  Time: 0.474s, 1081.01/s  (0.489s, 1046.46/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [1650/2502 ( 66%)]  Loss: 4.603 (5.48)  Time: 0.478s, 1071.60/s  (0.489s, 1047.49/s)  LR: 4.000e-01  Data: 0.033 (0.028)
Train: 0 [1700/2502 ( 68%)]  Loss: 4.424 (5.45)  Time: 0.467s, 1095.36/s  (0.488s, 1048.46/s)  LR: 4.000e-01  Data: 0.021 (0.028)
Train: 0 [1750/2502 ( 70%)]  Loss: 4.600 (5.42)  Time: 0.473s, 1083.40/s  (0.488s, 1049.35/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [1800/2502 ( 72%)]  Loss: 4.540 (5.40)  Time: 0.477s, 1072.53/s  (0.488s, 1050.16/s)  LR: 4.000e-01  Data: 0.028 (0.028)
Train: 0 [1850/2502 ( 74%)]  Loss: 4.485 (5.37)  Time: 0.466s, 1097.98/s  (0.487s, 1051.01/s)  LR: 4.000e-01  Data: 0.021 (0.028)
Train: 0 [1900/2502 ( 76%)]  Loss: 4.365 (5.35)  Time: 0.473s, 1083.21/s  (0.487s, 1051.81/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [1950/2502 ( 78%)]  Loss: 4.497 (5.33)  Time: 0.476s, 1075.50/s  (0.486s, 1052.47/s)  LR: 4.000e-01  Data: 0.030 (0.028)
Train: 0 [2000/2502 ( 80%)]  Loss: 4.446 (5.30)  Time: 0.476s, 1074.62/s  (0.486s, 1053.13/s)  LR: 4.000e-01  Data: 0.030 (0.028)
Train: 0 [2050/2502 ( 82%)]  Loss: 4.319 (5.28)  Time: 0.473s, 1082.36/s  (0.486s, 1053.73/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [2100/2502 ( 84%)]  Loss: 4.334 (5.26)  Time: 0.476s, 1074.94/s  (0.486s, 1054.32/s)  LR: 4.000e-01  Data: 0.031 (0.028)
Train: 0 [2150/2502 ( 86%)]  Loss: 4.336 (5.24)  Time: 0.475s, 1079.03/s  (0.485s, 1054.95/s)  LR: 4.000e-01  Data: 0.028 (0.028)
Train: 0 [2200/2502 ( 88%)]  Loss: 4.232 (5.21)  Time: 0.473s, 1082.67/s  (0.485s, 1055.48/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [2250/2502 ( 90%)]  Loss: 4.138 (5.19)  Time: 0.475s, 1078.94/s  (0.485s, 1055.95/s)  LR: 4.000e-01  Data: 0.029 (0.028)
Train: 0 [2300/2502 ( 92%)]  Loss: 4.155 (5.17)  Time: 0.471s, 1086.49/s  (0.485s, 1056.47/s)  LR: 4.000e-01  Data: 0.025 (0.028)
Train: 0 [2350/2502 ( 94%)]  Loss: 4.200 (5.15)  Time: 0.474s, 1079.44/s  (0.484s, 1056.96/s)  LR: 4.000e-01  Data: 0.027 (0.028)
Train: 0 [2400/2502 ( 96%)]  Loss: 4.218 (5.13)  Time: 0.475s, 1077.11/s  (0.484s, 1057.43/s)  LR: 4.000e-01  Data: 0.029 (0.028)
Train: 0 [2450/2502 ( 98%)]  Loss: 4.085 (5.11)  Time: 0.476s, 1075.42/s  (0.484s, 1057.88/s)  LR: 4.000e-01  Data: 0.030 (0.028)
Train: 0 [2500/2502 (100%)]  Loss: 4.130 (5.09)  Time: 0.471s, 1086.68/s  (0.484s, 1058.24/s)  LR: 4.000e-01  Data: 0.026 (0.028)
Train: 0 [2501/2502 (100%)]  Loss: 4.135 (5.09)  Time: 0.445s, 1150.52/s  (0.484s, 1058.28/s)  LR: 4.000e-01  Data: 0.000 (0.028)
Test: [   0/97]  Time: 4.504 (4.504)  Loss:  2.7930 (2.7930)  Acc@1: 34.1797 (34.1797)  Acc@5: 67.5781 (67.5781)
Test: [  50/97]  Time: 0.152 (0.311)  Loss:  4.2969 (3.5408)  Acc@1: 16.9922 (25.9344)  Acc@5: 37.8906 (50.5591)
Test: [  97/97]  Time: 1.334 (0.273)  Loss:  3.1289 (3.6528)  Acc@1: 33.3333 (25.3640)  Acc@5: 60.4167 (49.1140)
/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument "destination" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Current checkpoints:
 ('./output/train/20230220-213629-resnet50d-224/checkpoint-0.pth.tar', 25.363999991455078)

*** Best metric: 25.363999991455078 (epoch 0)
Total runtime (s): 1270
 
 
+------------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES REQUESTED | 
+------------------------------------------+ 
 
mem=372gb,walltime=02:00:00,ncpus=56
 
 
+-------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES USED | 
+-------------------------------------+ 
 
cput=02:14:46,mem=167048684kb,walltime=00:23:41,ncpus=56,cpupercent=569,vmem=1129868760kb
 
 
