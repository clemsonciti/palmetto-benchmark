---------------------------------
Using CONDA runtime.
---------------------------------
Training with a single process on 1 GPUs.
Model resnet50d created, param count:25576264
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 1
Train: 0 [   0/2502 (  0%)]  Loss: 6.977 (6.98)  Time: 17.178s,   29.81/s  (17.178s,   29.81/s)  LR: 4.000e-01  Data: 4.764 (4.764)
Train: 0 [  50/2502 (  2%)]  Loss: 6.745 (6.93)  Time: 0.504s, 1015.88/s  (0.831s,  615.79/s)  LR: 4.000e-01  Data: 0.029 (0.124)
Train: 0 [ 100/2502 (  4%)]  Loss: 6.603 (6.78)  Time: 0.503s, 1018.66/s  (0.670s,  764.66/s)  LR: 4.000e-01  Data: 0.027 (0.077)
Train: 0 [ 150/2502 (  6%)]  Loss: 6.448 (6.69)  Time: 0.505s, 1014.34/s  (0.616s,  831.81/s)  LR: 4.000e-01  Data: 0.028 (0.062)
Train: 0 [ 200/2502 (  8%)]  Loss: 6.349 (6.62)  Time: 0.507s, 1009.89/s  (0.589s,  869.94/s)  LR: 4.000e-01  Data: 0.029 (0.054)
Train: 0 [ 250/2502 ( 10%)]  Loss: 6.299 (6.55)  Time: 0.506s, 1012.08/s  (0.572s,  894.44/s)  LR: 4.000e-01  Data: 0.027 (0.049)
Train: 0 [ 300/2502 ( 12%)]  Loss: 6.188 (6.49)  Time: 0.506s, 1010.88/s  (0.562s,  911.36/s)  LR: 4.000e-01  Data: 0.028 (0.046)
Train: 0 [ 350/2502 ( 14%)]  Loss: 6.036 (6.43)  Time: 0.506s, 1010.92/s  (0.554s,  923.82/s)  LR: 4.000e-01  Data: 0.027 (0.043)
Train: 0 [ 400/2502 ( 16%)]  Loss: 5.959 (6.38)  Time: 0.507s, 1009.10/s  (0.549s,  933.31/s)  LR: 4.000e-01  Data: 0.027 (0.042)
Train: 0 [ 450/2502 ( 18%)]  Loss: 5.923 (6.33)  Time: 0.510s, 1004.70/s  (0.544s,  940.85/s)  LR: 4.000e-01  Data: 0.027 (0.040)
Train: 0 [ 500/2502 ( 20%)]  Loss: 5.758 (6.28)  Time: 0.509s, 1006.77/s  (0.541s,  946.96/s)  LR: 4.000e-01  Data: 0.029 (0.039)
Train: 0 [ 550/2502 ( 22%)]  Loss: 5.697 (6.23)  Time: 0.508s, 1006.90/s  (0.538s,  951.88/s)  LR: 4.000e-01  Data: 0.028 (0.038)
Train: 0 [ 600/2502 ( 24%)]  Loss: 5.586 (6.18)  Time: 0.508s, 1006.93/s  (0.535s,  956.15/s)  LR: 4.000e-01  Data: 0.028 (0.037)
Train: 0 [ 650/2502 ( 26%)]  Loss: 5.499 (6.14)  Time: 0.509s, 1006.43/s  (0.533s,  959.76/s)  LR: 4.000e-01  Data: 0.028 (0.037)
Train: 0 [ 700/2502 ( 28%)]  Loss: 5.451 (6.10)  Time: 0.509s, 1006.84/s  (0.532s,  962.87/s)  LR: 4.000e-01  Data: 0.028 (0.036)
Train: 0 [ 750/2502 ( 30%)]  Loss: 5.400 (6.05)  Time: 0.508s, 1007.28/s  (0.530s,  965.59/s)  LR: 4.000e-01  Data: 0.029 (0.036)
Train: 0 [ 800/2502 ( 32%)]  Loss: 5.236 (6.01)  Time: 0.509s, 1006.74/s  (0.529s,  967.99/s)  LR: 4.000e-01  Data: 0.028 (0.035)
Train: 0 [ 850/2502 ( 34%)]  Loss: 5.269 (5.97)  Time: 0.508s, 1008.42/s  (0.528s,  970.09/s)  LR: 4.000e-01  Data: 0.028 (0.035)
Train: 0 [ 900/2502 ( 36%)]  Loss: 5.196 (5.93)  Time: 0.508s, 1008.81/s  (0.527s,  971.98/s)  LR: 4.000e-01  Data: 0.028 (0.035)
Train: 0 [ 950/2502 ( 38%)]  Loss: 5.347 (5.90)  Time: 0.509s, 1006.56/s  (0.526s,  973.67/s)  LR: 4.000e-01  Data: 0.029 (0.034)
Train: 0 [1000/2502 ( 40%)]  Loss: 5.138 (5.86)  Time: 0.507s, 1010.02/s  (0.525s,  975.22/s)  LR: 4.000e-01  Data: 0.028 (0.034)
Train: 0 [1050/2502 ( 42%)]  Loss: 4.929 (5.82)  Time: 0.508s, 1007.30/s  (0.524s,  976.63/s)  LR: 4.000e-01  Data: 0.027 (0.034)
Train: 0 [1100/2502 ( 44%)]  Loss: 5.043 (5.79)  Time: 0.509s, 1005.41/s  (0.524s,  977.86/s)  LR: 4.000e-01  Data: 0.028 (0.034)
Train: 0 [1150/2502 ( 46%)]  Loss: 5.043 (5.75)  Time: 0.507s, 1009.03/s  (0.523s,  979.00/s)  LR: 4.000e-01  Data: 0.028 (0.033)
Train: 0 [1200/2502 ( 48%)]  Loss: 4.966 (5.72)  Time: 0.509s, 1006.85/s  (0.522s,  980.06/s)  LR: 4.000e-01  Data: 0.028 (0.033)
Train: 0 [1250/2502 ( 50%)]  Loss: 4.847 (5.69)  Time: 0.510s, 1003.00/s  (0.522s,  981.03/s)  LR: 4.000e-01  Data: 0.029 (0.033)
Train: 0 [1300/2502 ( 52%)]  Loss: 4.857 (5.66)  Time: 0.508s, 1008.84/s  (0.521s,  981.93/s)  LR: 4.000e-01  Data: 0.028 (0.033)
Train: 0 [1350/2502 ( 54%)]  Loss: 4.767 (5.63)  Time: 0.509s, 1006.62/s  (0.521s,  982.73/s)  LR: 4.000e-01  Data: 0.029 (0.033)
Train: 0 [1400/2502 ( 56%)]  Loss: 4.744 (5.60)  Time: 0.510s, 1004.28/s  (0.521s,  983.52/s)  LR: 4.000e-01  Data: 0.029 (0.033)
Train: 0 [1450/2502 ( 58%)]  Loss: 4.666 (5.57)  Time: 0.507s, 1009.53/s  (0.520s,  984.22/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1500/2502 ( 60%)]  Loss: 4.731 (5.54)  Time: 0.508s, 1007.71/s  (0.520s,  984.88/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1550/2502 ( 62%)]  Loss: 4.831 (5.51)  Time: 0.509s, 1006.19/s  (0.520s,  985.50/s)  LR: 4.000e-01  Data: 0.029 (0.032)
Train: 0 [1600/2502 ( 64%)]  Loss: 4.769 (5.48)  Time: 0.507s, 1010.54/s  (0.519s,  986.06/s)  LR: 4.000e-01  Data: 0.027 (0.032)
Train: 0 [1650/2502 ( 66%)]  Loss: 4.605 (5.45)  Time: 0.508s, 1008.05/s  (0.519s,  986.61/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1700/2502 ( 68%)]  Loss: 4.421 (5.43)  Time: 0.508s, 1008.52/s  (0.519s,  987.14/s)  LR: 4.000e-01  Data: 0.029 (0.032)
Train: 0 [1750/2502 ( 70%)]  Loss: 4.627 (5.40)  Time: 0.509s, 1005.21/s  (0.518s,  987.64/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1800/2502 ( 72%)]  Loss: 4.527 (5.38)  Time: 0.508s, 1007.03/s  (0.518s,  988.11/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1850/2502 ( 74%)]  Loss: 4.462 (5.35)  Time: 0.508s, 1008.38/s  (0.518s,  988.55/s)  LR: 4.000e-01  Data: 0.028 (0.032)
Train: 0 [1900/2502 ( 76%)]  Loss: 4.341 (5.33)  Time: 0.509s, 1005.70/s  (0.518s,  988.95/s)  LR: 4.000e-01  Data: 0.029 (0.032)
Train: 0 [1950/2502 ( 78%)]  Loss: 4.482 (5.31)  Time: 0.506s, 1011.00/s  (0.518s,  989.35/s)  LR: 4.000e-01  Data: 0.027 (0.032)
Train: 0 [2000/2502 ( 80%)]  Loss: 4.437 (5.28)  Time: 0.510s, 1004.36/s  (0.517s,  989.72/s)  LR: 4.000e-01  Data: 0.030 (0.032)
Train: 0 [2050/2502 ( 82%)]  Loss: 4.347 (5.26)  Time: 0.509s, 1004.92/s  (0.517s,  990.06/s)  LR: 4.000e-01  Data: 0.029 (0.032)
Train: 0 [2100/2502 ( 84%)]  Loss: 4.333 (5.24)  Time: 0.509s, 1005.90/s  (0.517s,  990.38/s)  LR: 4.000e-01  Data: 0.029 (0.032)
Train: 0 [2150/2502 ( 86%)]  Loss: 4.368 (5.22)  Time: 0.509s, 1006.36/s  (0.517s,  990.68/s)  LR: 4.000e-01  Data: 0.029 (0.031)
Train: 0 [2200/2502 ( 88%)]  Loss: 4.229 (5.20)  Time: 0.509s, 1006.56/s  (0.517s,  990.95/s)  LR: 4.000e-01  Data: 0.028 (0.031)
Train: 0 [2250/2502 ( 90%)]  Loss: 4.145 (5.18)  Time: 0.509s, 1005.05/s  (0.517s,  991.21/s)  LR: 4.000e-01  Data: 0.029 (0.031)
Train: 0 [2300/2502 ( 92%)]  Loss: 4.163 (5.16)  Time: 0.511s, 1002.75/s  (0.516s,  991.48/s)  LR: 4.000e-01  Data: 0.031 (0.031)
Train: 0 [2350/2502 ( 94%)]  Loss: 4.224 (5.14)  Time: 0.509s, 1006.72/s  (0.516s,  991.74/s)  LR: 4.000e-01  Data: 0.028 (0.031)
Train: 0 [2400/2502 ( 96%)]  Loss: 4.218 (5.12)  Time: 0.508s, 1007.67/s  (0.516s,  991.98/s)  LR: 4.000e-01  Data: 0.028 (0.031)
Train: 0 [2450/2502 ( 98%)]  Loss: 4.087 (5.10)  Time: 0.511s, 1002.24/s  (0.516s,  992.20/s)  LR: 4.000e-01  Data: 0.031 (0.031)
Train: 0 [2500/2502 (100%)]  Loss: 4.175 (5.08)  Time: 0.507s, 1009.27/s  (0.516s,  992.44/s)  LR: 4.000e-01  Data: 0.028 (0.031)
Train: 0 [2501/2502 (100%)]  Loss: 4.118 (5.08)  Time: 0.478s, 1070.91/s  (0.516s,  992.47/s)  LR: 4.000e-01  Data: 0.000 (0.031)
Test: [   0/97]  Time: 5.392 (5.392)  Loss:  2.8262 (2.8262)  Acc@1: 35.9375 (35.9375)  Acc@5: 69.5312 (69.5312)
Test: [  50/97]  Time: 0.161 (0.380)  Loss:  4.2070 (3.4624)  Acc@1: 21.4844 (27.1561)  Acc@5: 41.6016 (52.9259)
Test: [  97/97]  Time: 1.379 (0.333)  Loss:  3.6289 (3.6352)  Acc@1: 25.5952 (25.5420)  Acc@5: 48.8095 (49.7900)
Current checkpoints:
 ('./output/train/20230220-170046-resnet50d-224/checkpoint-0.pth.tar', 25.541999997558595)

*** Best metric: 25.541999997558595 (epoch 0)
Total runtime (s): 1349
 
 
+------------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES REQUESTED | 
+------------------------------------------+ 
 
mem=372gb,walltime=02:00:00,ncpus=56
 
 
+-------------------------------------+ 
| PALMETTO CLUSTER PBS RESOURCES USED | 
+-------------------------------------+ 
 
cput=02:59:18,mem=13992820kb,walltime=00:22:31,ncpus=56,cpupercent=730,vmem=1104811276kb
 
 
